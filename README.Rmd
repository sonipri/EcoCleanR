---
output: github_document
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "man/figures/README-",
  fig.width = 8,
  fig.height = 6,
  out.width = "70%"
)
```

# EcoCleanR V1.0

<!-- badges: start -->
[![Codecov test coverage](https://codecov.io/gh/sonipri/EcoCleanR/graph/badge.svg)](https://app.codecov.io/gh/sonipri/EcoCleanR)
<!-- badges: end -->

**The goal of EcoCleanR is to provides functions to integrate biodiversity data from multiple aggregates and offers a step-by-step framework for data cleaning, outlier detection, and the generation of species biogeographic ranges. It also supports the extraction of relevant environmental information to assist in ecological and biogeographic analyses.**

Key features:<br>

1. Data Merging: Merge data from GBIF (with *rgbif*), iDigBio (with *ridigbio*), OBIS (with *robis*) and local input data file (e.g. used csv file downloaded from InvertEBase database). <br>
2. Data Cleaning: <br>
2.1. Remove duplicates<br>
2.2. Check bad taxon using WoRMs<br>
2.3. Improve the coordinate information utilizing external georeference tools<br>
2.4. check the coordinate precision and rounding<br>
2.5. Flag the records associated with the wrong ocean/sea and inland<br>
2.6. Extract the environmental variables<br>
2.7. Impute the environmental variables if no assignment from online resources<br>
2.8. Identifying outliers<br>
3. Data visualization: <br>
3.1. Create map view<br>
3.2. Execute a summary table with max, min and mean limits of variables<br>
3.3. Create a summary and plot to represent suitable range of a species for spatial and non spatial attributes<br>

# Installation from GitHub

Install the development version of *EcoCleanR* from [GitHub](https://github.com/) with:
https://github.com/sonipri/EcoCleanR/<br>

How to install package through zip file:<br>

 a) Install *devtools* and *remotes* packages and load them.<br>

      #install.packages("devtools")<br>
      #install.packages("remotes")<br>
        #library(devtools)<br>
        #library(remotes)<br>
      
 

 b) Install *EcoCleanR* package from zip file.<br>

      #devtools::install_local("EcoCleanR-xxx.zip",         dependencies = TRUE)                   

 c) unzip the zip file EcoCleanR and set it as working directory.<br>

      #setwd(...path)
      

 d) *Optional: Only for testing purposes*. <br>
 
      #devtools::check()

    
 e) see data merging steps vignette: [`data_merging`]<br>
   
    see data cleaning steps on merged dataset at vignette: [`data_cleaning`]<br>
   
   **Recommanded** :see the Step-by-Step Workflow for complete process — from data downloading, processing, merging, cleaning, and visualization at vignettes/article/stepbystep.rmd. 

# Example

This is a basic example to demonstrate data processing, merging and cleanings steps for a species name "Mexacanthina lugubris"

```{r example}
library(EcoCleanR)
## basic example code
# After execution of step 1 and 2.1, output file called "ecodata"
# This dataset contains ~1100 occurrence records for the species Mexacanthina lugubris. It was compiled by merging data from multiple online repositories — GBIF, OBIS, iDigBio and local file (InvertEbase) — consider as step 1 (Vignette: data_merging). Duplicate records were removed to retain only unique occurrences (2.1) using function "ec_rm_duplicate".
# The algorithm for these steps can be found on article folder - documentation
head(ecodata)

# visualizing the raw data
ec_geographic_map(ecodata)

# lets execute cleaning steps
# step 2.2 - check taxon error for species name - "Mexacanthina lugubris"
comparison <- ec_worms_synonym("Mexacanthina lugubris", ecodata)
# step 2.3 - check records with locality but no coordinate assignments
ecodata$flag_with_locality <- ec_flag_with_locality(ecodata)
# upload back the corrected file with csv upload - name it as ecodata_corrected
head(ecodata_corrected)
ecodata <- ec_merge_corrected_coordinates(ecodata_corrected, ecodata)

ecodata <- ec_filter_by_uncertainty(ecodata, uncertainty_col = "coordinateUncertaintyInMeters", percentile = 0.95, ask = TRUE)

# step 2.4 - check records with bad precision <2 as well as rounding issue
ecodata$flag_precision <- ec_flag_precision(ecodata$decimalLongitude, ecodata$decimalLatitude)
# step 2.5 - check records with wrong assignment of ocean/sea and inland with certain buffer range - not executing on run
if (FALSE) {
  ecodata$flag_non_region <- ec_flag_non_region("east", "pacific", buffer = 25000, ecodata)
}
# step 2.6 - extract the environmental variables, here we are using a data table which has unique combination of coordinates - we call it ecodata_x - not executing on run
if (FALSE) {
  ecodata_x <- ec_extract_env_layers(ecodata_x, env_layers = env_layers)
}
# step 2.7 - impute the environmental variables for those coordinates with no assignment on online data sources.
if (FALSE) {
  ecodata_x <- ec_impute_env_values(ecodata_x, radius_km, iter)
}
# step 2.8 - calculate outlier probability for each data points.
if (FALSE) {
  ecodata_x$flag_outliers <- ((ec_flag_outlier(ecodata_x, env_layers, itr = 100, k = 3, geo_quantile = 0.99, maha_quantile = 0.99)))$ouliers
}
# step 3.1 - visualize the map with outlier probability index, bind the ecodata_x with ecodata with updated column flag_outlier
ec_geographic_map_w_flag(ecodata_with_outliers, flag_column = "outliers")
# at this stage we can decide the acceptable outlier probability and after removing higher probable a new datafram called ecodata_cleaned can be source of summary table (next step)
# step 3.2 - Visualize the summary table
env_layers <- c("BO_sstmean", "BO_sstmax", "BO_sstmin")
data("ecodata_cleaned")
ec_geographic_map(ecodata_cleaned)

summary_table <- ec_var_summary(ecodata_cleaned, env_layers)
print(summary_table)
# step 3.3 - Plot to visual the acceptable limit of a species which demonstrate a suitable habitat range:
ec_plot_var_range(ecodata_with_outliers, summary_table, env_layers)
```

Further documents:<br>

*see data merging vignette: [`data_merging`]<br>

*see data cleaning steps on merged dataset at vignette: [`data_cleaning`]<br>

*see the Step-by-Step Workflow vignette for a detailed explanation of the complete process — from data downloading to merging, cleaning, and visualization at vignettes/article/stepbystep.rmd.<br>

*see citation guidelines for the downloaded data from gbif, obis, idigbio and InvertEbase [cite_data] at vignettes/article/cite_data.rmd.
