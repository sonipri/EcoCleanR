---
title: "data_cleaning"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{data_cleaning}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  eval = FALSE
)
```

```{r setup}
library(EcoCleanR)
```

#Background

In this tutorial we will show cleaning of merge datafile

#Code execution:
```{r libraries}
# Install packages
# library(devtools)
# library(taxize)
library(dplyr)
# library(ggplot2)
# library(patchwork)
# library(sf)
# library(geodata)
# library(geosphere)
# devtools::install_github("ropensci/mregions2", build_vignettes = TRUE)
# library(mregions2)
# library(sdmpredictors)
# library(terra)
# library(tidyr)
# library(rlang)
# end of install package
species_name <- "Mexacanthina lugubris"
# Step1: remove duplicates
# ecodata <- ec_rm_duplicate(Mixdb.occ, catalogNumber = "catalogNumber", abundance = "abundance")
# str(ecodata[,1:3])
### end of remove duplicates

### Optional to perform: check the institution code
#inst_counts <- ecodata_cl %>%
  #group_by(institutionCode) %>%
  #summarise(record_count = n(), .groups = "drop")

### optional to perform..
#ecodata_cl <- ecodata_cl %>%
  #filter(is.na(institutionCode) | institutionCode != "NRM")




# Step2: check taxon error - ec_worms_synonym function
comparison <- ec_worms_synonym(species_name, ecodata)
print(comparison)
### identified bad taxon... filter it
ecodata_cl <- ecodata %>%
  filter(scientificName != "Morula lugubris (I.Sowerby, 1822)")
str(ecodata_cl[, 1:3])

##ec_itis_synonym can be used to check synonyms from Integrated Taxonomic Information System. eg. terrestrial fauna.
## Currently synonym check is restricted to WoRMS and ITIS taxonomy databases.
### End of taxon check

# Step3: check records for georeferencing - ec_flag_with_locality

# codata_cl$flag_check_geolocate <- ec_flag_with_locality(ecodata_cl)
# tr(ecodata_cl[,1:3])
# data_need_correction <- ecodata %>%
# filter(flag_check_geolocate!= 1)
# write.csv(data_need_correction, "data_check_geolocate.csv") ###excute this code when required.

# ecodata_corrected <- read.csv("M lugubris_corrected_geolocate.csv")###comment this code as we have this file in "/data" folder

# codata_cl <- ec_merge_corrected_coordinates(ecodata_corrected, ecodata_cl)

# tr(ecodata_cl[,1:3]) ###merged records with improved georeference.
# c_geographic_map(ecodata_cl)###plot the map

ecodata_cl <- ec_filter_by_uncertainty(ecodata_cl, uncertainty_col = "coordinateUncertaintyInMeters", percentile = 0.95, ask = FALSE)

str(ecodata_cl[, 1:3])
ec_geographic_map(ecodata_cl) ### plot the map
### end of ec_flag_with locality

# Step4: flag with poor precision - ec_flag_precision
ecodata_cl$flag_precision <- ec_flag_precision(ecodata_cl$decimalLongitude, ecodata_cl$decimalLatitude)
### filter the flag - flag_cordinate_precision
ecodata_cl <- ecodata_cl %>%
  filter(flag_precision != 1) # remove flagged records
str(ecodata_cl[1:3])
### end of flag with poor precision

# Step5: flag recored from wrong ocean/sea - ec_flag_non_region
direction <- "east"
buffer <- 25000
ocean <- "pacific"
ecodata_cl$flag_non_region <- ec_flag_non_region(direction, ocean, buffer, ecodata_cl)

str(ecodata_cl[, 1:3])
ec_geographic_map_w_flag(ecodata_cl, flag_column = "flag_non_region") # map view with flagged records
ecodata_cl <- ecodata_cl %>%
  filter(flag_non_region != 1) # remove flagged records
ec_geographic_map(ecodata_cl)

### end of flaggin recored from wrong ocean/sea - ec_flag_non_region


# Step6: extract the environmental data
### get the unique combination of coordiantes
ecodata_unique <- ecodata_cl[, c("decimalLatitude", "decimalLongitude")]
ecodata_unique <- base::unique(ecodata_unique)
available_layers <- list_layers() # returns something like c("BO_sstmean", "BO_sstmax", ...) 
#get the layer_code from the available_layers table, please make sure to put exact name on the variable env_layers

env_layers <- c("BO_sstmean", "BO_sstmin", "BO_sstmax")


### extraction env layers
ecodata_unique <- ec_extract_env_layers(ecodata_unique, env_layers = env_layers) # warning message, extraction of env data from the cache
### impute env var values those were missing after extraction
ecodata_unique <- ec_impute_env_values(ecodata_unique, radius_km = 10, iter = 3)

# Step7: Falgging outliers - ec_flag_outliers
### omit the coordinate which couldn't get any env values after imputation
ecodata_unique <- na.omit(ecodata_unique)

ecodata_unique$flag_outliers <- ec_flag_outlier(ecodata_unique, env_layers, itr = 100, k = 3, geo_quantile = 0.99, maha_quantile = 0.99)$outlier
### this function gives a list view, list one has all the 100 iteration dataframs and list 2 is consolidated outlier list derived from all the 100 iterations

### merge back the env variables and outliers into main table
ecodata_cl <- ecodata_cl %>%
  left_join(ecodata_unique[, c("decimalLatitude", "decimalLongitude", "flag_outliers", env_layers)],
    by = c("decimalLatitude", "decimalLongitude")
  )

ec_geographic_map_w_flag(ecodata_cl, flag_column = "flag_outliers")
### consider a threshold to accept a data point outlier with the outlier probability >0.8
cleaned_data <- ecodata_cl %>%
  filter(flag_outliers < 0.95)
ec_geographic_map(cleaned_data)
# Step8: Create summary table for the accepted occurrence record to define a species distribution range
summary_table <- ec_var_summary(cleaned_data, env_layers = env_layers)
head(summary_table)
# Step9: Create plot to show accepted data records
ec_plot_var_range(ecodata_cl, summary_df = summary_table, env_layers = env_layers)
```
